
import re
from collections import Counter
import langid
from nltk.tokenize import word_tokenize
import nltk
nltk.download('punkt')
nltk.download('punkt_tab')
nltk.download('stopwords')
import spacy
import string
from textblob import TextBlob
from transformers import MarianMTModel, MarianTokenizer
from sklearn.metrics.pairwise import cosine_similarity
from transformers import pipeline
from sentence_transformers import SentenceTransformer, util

def word_frequency(text):
    text = text.lower()
    words = re.findall(r'\b\w+\b', text)
    return Counter(words)



#1
teksts = ("MÄkoÅ†ainÄ dienÄ kaÄ·is sÄ“dÄ“ja uz palodzes. "
            "KaÄ·is domÄja, kÄpÄ“c debesis ir pelÄ“kas. "
            "KaÄ·is gribÄ“ja redzÄ“t sauli, bet saule slÄ“pÄs aiz mÄkoÅ†iem.")
frekvences = word_frequency(teksts)
for vÄrds, skaits in frekvences.items():
    print(f"{vÄrds}: {skaits}")


    
#2

texts = [
    "Å odien ir saulaina diena.",
    "Today is a sunny day.",
    "Ğ¡ĞµĞ³Ğ¾Ğ´Ğ½Ñ ÑĞ¾Ğ»Ğ½ĞµÑ‡Ğ½Ñ‹Ğ¹ Ğ´ĞµĞ½ÑŒ."
]
for text in texts:
    language, confidence = langid.classify(text)
    print(f"Teksts: \"{text}\" - Valoda: {language}")



#3

def preprocess_text(text, lang='latvian'):
    words = word_tokenize(text.lower())
    
    words = [word for word in words if word not in string.punctuation]
    
    latvian_stopwords = {'ir', 'un', 'to', 'klÄj', 'aiz', 'bet'}
    stop_words = set(latvian_stopwords)
    words = [word for word in words if word not in stop_words]
    
    return words

def calculate_similarity(text1, text2, lang='latvian'):
    words1 = preprocess_text(text1, lang)
    words2 = preprocess_text(text2, lang)
    
    set1 = set(words1)
    set2 = set(words2)
    
    common_words = set1.intersection(set2)
    
    total_unique_words = set1.union(set2)
    similarity_percentage = (len(common_words) / len(total_unique_words)) * 100
    
    return common_words, similarity_percentage

text1 = "Rudens lapas ir dzeltenas un oranÅ¾as. Lapas klÄj zemi un padara to krÄsainu."
text2 = "KrÄsainas rudens lapas krÄ«t zemÄ“. Lapas ir oranÅ¾as un dzeltenas."

common, percentage = calculate_similarity(text1, text2)

print(f"SakritÄ«gie vÄrdi: {', '.join(common)}")
print(f"SakritÄ«bas lÄ«menis: {percentage:.2f}%")



#4

def analyze_sentiment(sentence):
    translations = {
        "Å is produkts ir lielisks, esmu Ä¼oti apmierinÄts!": "This product is great, I am very satisfied!",
        "Esmu vÄ«lies, produkts neatbilst aprakstam.": "I am disappointed, the product does not match the description.",
        "NeitrÄls produkts, nekas Ä«paÅ¡s.": "Neutral product, nothing special."
    }
    english_sentence = translations[sentence]
    
    blob = TextBlob(english_sentence)
    polarity = blob.sentiment.polarity  

    print(polarity)
    
    if polarity > 0.5:
        sentiment = "pozitÄ«vs"
    elif polarity < 0:
        sentiment = "negatÄ«vs"
    else:
        sentiment = "neitrÄls"
    
    return sentiment

sentences = [
    "Å is produkts ir lielisks, esmu Ä¼oti apmierinÄts!",
    "Esmu vÄ«lies, produkts neatbilst aprakstam.",
    "NeitrÄls produkts, nekas Ä«paÅ¡s."
]

for sentence in sentences:
    sentiment = analyze_sentiment(sentence)
    print(f"Teikums: \"{sentence}\" - NoskaÅ†ojums: {sentiment}")



#5

def clean_text(text):
    text = re.sub(r"@\w+", "", text)
    text = re.sub(r"http\S+|www\S+|https\S+", "", text, flags=re.MULTILINE)
    text = re.sub(r"[^\w\s]", "", text)
    text = text.lower()
    text = re.sub(r"\s+", " ", text).strip()
    return text

raw_text = "@John: Å is ir lielisks produkts!!! Vai ne? ğŸ‘ğŸ‘ğŸ‘ http://example.com"
cleaned_text = clean_text(raw_text)
print("Cleaned Text:", cleaned_text)



#6

def summarize_text(article, max_length=50, min_length=25):
    summarizer = pipeline("summarization", model="facebook/bart-large-cnn")
    summary = summarizer(article, max_length=max_length, min_length=min_length, do_sample=False)
    return summary[0]['summary_text']

article = """
Latvija ir valsts Baltijas reÄ£ionÄ. TÄs galvaspilsÄ“ta ir RÄ«ga, kas ir slavena ar savu vÄ“sturisko centru un skaistajÄm Ä“kÄm. Latvija robeÅ¾ojas ar Lietuvu, Igauniju un Krieviju, kÄ arÄ« tai ir piekÄ¼uve Baltijas jÅ«rai. TÄ ir viena no Eiropas SavienÄ«bas dalÄ«bvalstÄ«m.
"""

summary = summarize_text(article)
print("Summary:", summary)


#7


model = SentenceTransformer('distiluse-base-multilingual-cased-v1')

words = ["mÄja", "dzÄ«voklis", "jÅ«ra"]

embeddings = model.encode(words, convert_to_tensor=True)

similarity_matrix = util.pytorch_cos_sim(embeddings, embeddings)

print("VÄrdu lÄ«dzÄ«bas:")
for i in range(len(words)):
    for j in range(i + 1, len(words)):
        sim = similarity_matrix[i][j].item()
        print(f"LÄ«dzÄ«ba starp '{words[i]}' un '{words[j]}': {sim:.4f}")


#8
nlp = spacy.load("xx_ent_wiki_sm") 

text = "Valsts prezidents Egils Levits piedalÄ«jÄs pasÄkumÄ, ko organizÄ“ja Latvijas UniversitÄte."

doc = nlp(text)

person_entities = []
org_entities = []

for ent in doc.ents:
    if ent.label_ == "PER":
        if ent.text.endswith("UniversitÄte"):
            org_entities.append(ent.text)
        else:
            person_entities.append(ent.text)
    elif ent.label_ == "ORG":
        org_entities.append(ent.text)

print("PersonvÄrdi:")
for person in person_entities:
    print(f"- {person}")

print("\nOrganizÄcijas:")
for org in org_entities:
    print(f"- {org}")

#9
def generate_story(starting_phrase, max_length=50, num_return_sequences=1):
    generator = pipeline("text-generation", model="gpt2")
    
    generated_texts = generator(
        starting_phrase, 
        max_length=max_length, 
        num_return_sequences=num_return_sequences,
        do_sample=True, 
        top_k=50,      
        top_p=0.95      
    )
    
    return [text['generated_text'] for text in generated_texts]

starting_phrase = "Reiz kÄdÄ tÄlÄ zemÄ“..."

generated_stories = generate_story(starting_phrase, max_length=50)

for i, story in enumerate(generated_stories):
    print(f"StÄsts {i + 1}: {story}")



#10

def translate(texts, src_lang="lv", tgt_lang="en"):
    model_name = f'Helsinki-NLP/opus-mt-{src_lang}-{tgt_lang}'
    
    tokenizer = MarianTokenizer.from_pretrained(model_name)
    model = MarianMTModel.from_pretrained(model_name)
    
    translated = model.generate(**tokenizer(texts, return_tensors="pt", padding=True))
    
    translated_texts = [tokenizer.decode(t, skip_special_tokens=True) for t in translated]
    
    return translated_texts

latv_texts = [
    "Labdien! KÄ jums klÄjas?",
    "Es Å¡odien lasÄ«ju interesantu grÄmatu."
]

anglu_texts = translate(latv_texts)

for lv, en in zip(latv_texts, anglu_texts):
    print(f"LatvieÅ¡u: {lv}")
    print(f"AngÄ¼u: {en}\n")
